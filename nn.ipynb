{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network implementation\n",
    "## Group 57\n",
    "\n",
    "Based on the Neural Network by Michael Neilsen: https://github.com/mnielsen/neural-networks-and-deep-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A neural network impelentation which uses stochastic gradient descent for a \n",
    "feedforward neural network.\n",
    "\"\"\"\n",
    "class Network(object):\n",
    "    \"\"\"\n",
    "    The list ``sizes`` contains the number of neurons in the\n",
    "    respective layers of the network.  For example, if the list\n",
    "    was [2, 3, 1] then it would be a three-layer network, with the\n",
    "    first layer containing 2 neurons, the second layer 3 neurons,\n",
    "    and the third layer 1 neuron. The first layer is the input, last the output\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = []\n",
    "        self.weights = []\n",
    "        for y in sizes[1:]:\n",
    "            self.biases.append(np.random.randn(y,1))\n",
    "        for x, y in zip(sizes[:-1], sizes[1:]):\n",
    "            self.weights.append(np.random.randn(y, x))\n",
    "    \"\"\"\n",
    "    a = activation\n",
    "    b = bias\n",
    "    w = weight\n",
    "    sigmoid : see sigmoid function\n",
    "\n",
    "    Role : Loops through the whole network and updates each neurons activation \n",
    "    using the sigmoid function\n",
    "    \"\"\"    \n",
    "    def feedforward(self, a):\n",
    "\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = self.activation(w, a, b)\n",
    "        return a\n",
    "    \"\"\"\n",
    "    Train the neural network using mini-batch stochastic gradient descent.\n",
    "    \"\"\"\n",
    "    def SGD(self, X_train, Y_train, X_validation, Y_validation, epochs, mini_batch_size, learning_rate, decay):\n",
    "\n",
    "        training_data = zip(X_train, Y_train)\n",
    "        validation_data = zip(X_validation, Y_validation)\n",
    "    \n",
    "        \"\"\"Take the training data and make a list out of it\"\"\"\n",
    "        training_data = list(training_data)\n",
    "        \n",
    "        \"\"\"Check if there is data in the test_data\"\"\"\n",
    "        if validation_data:\n",
    "            validation_data = list(validation_data)\n",
    "            n_validation_data = len(validation_data)\n",
    "        \n",
    "        \"\"\"\n",
    "        Mini-batches: Each mini-batch contains mini_batch_size elements from the training set. \n",
    "        Splits the training data into mini-batches, and for each mini-batches we train the network. \n",
    "        \"\"\"       \n",
    "        mini_batches = []\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            for k in range(0, len(training_data), mini_batch_size):\n",
    "                mini_batches.append(training_data[k:k+mini_batch_size])\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, learning_rate)\n",
    "            if validation_data:\n",
    "                print(\"Epoch #\" + str(j+1), end = '\\t')\n",
    "                self.evaluate(X_validation, Y_validation)\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j))\n",
    "            learning_rate = learning_rate * (1-decay)\n",
    "    \"\"\"\n",
    "    Update the network's weights and biases by applying\n",
    "    gradient descent using backpropagation to a single mini batch.\n",
    "    The ``mini_batch`` is a list of tuples ``(x, y)``.\n",
    "    \"\"\"\n",
    "    def update_mini_batch(self, mini_batch, learning_rate):\n",
    "        \"\"\"\n",
    "        Building an empty networks filled with empty 0 \n",
    "        \"\"\"\n",
    "        mini_batch_bias = []\n",
    "        mini_batch_weights = []\n",
    "        for b in self.biases:\n",
    "            mini_batch_bias.append(np.zeros(b.shape))\n",
    "        for w in self.weights:\n",
    "            mini_batch_weights.append(np.zeros(w.shape))\n",
    "        \n",
    "        \"\"\"\n",
    "        x: Features of the instance\n",
    "        y: Label of the instance\n",
    "        eta : Learning rate\n",
    "        Loops through the samples of the mini-batch, calls backprop on each sample. \n",
    "        \"\"\"\n",
    "        for x, y in mini_batch:\n",
    "            \n",
    "            \"\"\"\n",
    "            Returns the gradient of the loss function \n",
    "            \"\"\"\n",
    "            loss_func_bias, loss_func_weight = self.backprop(x, y)\n",
    "            \"\"\"\n",
    "            Updates the mini-batch bias and mini-batch weight by adding their respective loss function to the\n",
    "            current mini-batch's network\n",
    "            \"\"\"\n",
    "            mini_batch_bias = [ub+lfb for ub, lfb in zip(mini_batch_bias, loss_func_bias)]\n",
    "            mini_batch_weights = [uw+lfw for uw, lfw in zip(mini_batch_weights, loss_func_weight)]\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Updates each weight with the weights calculated in he minibatch\n",
    "        \"\"\"\n",
    "        self.weights = [ self.update_network_weights(current_weight, learning_rate, mini_batch, mini_batch_weight)\n",
    "                        for current_weight, mini_batch_weight in zip(self.weights, mini_batch_weights)]\n",
    "\n",
    "        \"\"\"\n",
    "        Updates each weight with the bias calculated in he minibach\n",
    "        \"\"\"    \n",
    "        self.biases = [self.update_network_bias(current_bias, learning_rate, mini_batch, mini_batch_bias)\n",
    "                       for current_bias, mini_batch_bias in zip(self.biases, mini_batch_bias)]\n",
    "    \"\"\"\n",
    "    Return a tuple (updated_bias, updated_weight) representing the\n",
    "    gradient for the cost function C_x. \n",
    "    \"\"\"     \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        Building an empty network filled with empty 0's\n",
    "        \"\"\" \n",
    "        updated_bias = []\n",
    "        for b in self.biases:\n",
    "            updated_bias.append(np.zeros(b.shape))        \n",
    "        \n",
    "        updated_weight = []\n",
    "        for w in self.weights:\n",
    "            updated_weight.append(np.zeros(w.shape))\n",
    "        \n",
    "        \"\"\"\n",
    "        FEED FORWARD\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Input layer's values\n",
    "        \"\"\"\n",
    "        activation = x\n",
    "    \n",
    "        \"\"\"\n",
    "        List to store all the activations, initialized with the input layer as the first layer\n",
    "        \"\"\"\n",
    "        activation_list = [x]\n",
    "        \n",
    "        \"\"\"\n",
    "        List to store all the vectorized output of each layer\n",
    "        \"\"\"\n",
    "        z_list = [] \n",
    "        \n",
    "        \"\"\"\n",
    "        z : vectorized output of each layer\n",
    "        z_list : List of vectorized outputs of each layer\n",
    "        activation : activation of a layer with sigmoid (sigmoid of the vectorized output)\n",
    "        activation_list : List of activation of each layer with sigmoid\n",
    "        \n",
    "        Run through the network and record the activation with and without sigmoid and save it in  \n",
    "        z_list(without) and activation_list(with)\n",
    "        \"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = self.vectorized_output(w, activation, b)\n",
    "            z_list.append(z)\n",
    "            activation = self.activation(w, activation, b)\n",
    "            activation_list.append(activation)         \n",
    "            \n",
    "        \"\"\"\n",
    "        Now that we ran through the network and calculated the activation we go backward through each layer \n",
    "        (from output to input) and update the bias and weight linked to the activations in order to get more \n",
    "        accurate results.\n",
    "        \"\"\"  \n",
    "        \n",
    "        \"\"\"\n",
    "        Apply output error formula on the last layer of the activation_list and z_list\n",
    "        \"\"\"\n",
    "        output_layer_error = self.output_layer_error(activation_list, z_list , y)\n",
    "        \n",
    "        \"\"\"\n",
    "        BACKPROPAGATION\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Update bias and weights of the last layer\n",
    "        \"\"\"\n",
    "        updated_bias[-1] = output_layer_error\n",
    "        updated_weight[-1] = np.dot(output_layer_error, activation_list[-2].transpose())\n",
    "        \n",
    "        output_error = output_layer_error\n",
    "        for l in range(2, self.num_layers):\n",
    "            updated_bias[-l] = self.output_error(l, updated_bias[-l+1] , z_list)\n",
    "            updated_weight[-l] = self.cost_weight(l, updated_bias[-l], activation_list)\n",
    "        return (updated_bias, updated_weight)       \n",
    "        \n",
    "    \"\"\"\n",
    "    Return the number of test inputs for which the neural\n",
    "    network outputs the correct result. Note that the neural\n",
    "    network's output is assumed to be the index of whichever\n",
    "    neuron in the final layer has the highest activation.\n",
    "    \"\"\"   \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "\n",
    "        test_data = zip(X_test, Y_test)\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        \n",
    "        result = (sum(int(x == y) for (x, y) in test_results) / 100)\n",
    "        print(\"Accuracy : \"+ str(result) + \"%\")\n",
    "    \n",
    "    \"\"\"\n",
    "    Mathematical Functions used in the network\n",
    "    \"\"\"\n",
    "    \n",
    "    def vectorized_output(self, w, a, b):\n",
    "        return np.dot(w, a)+b\n",
    "    \n",
    "    def activation(self, w, a, b):\n",
    "        return sigmoid(np.dot(w, a)+b)\n",
    "    \n",
    "    def output_layer_error(self, activation_list, z_list , y):\n",
    "        return (activation_list[-1] - y) * sigmoid_prime(z_list[-1])\n",
    "    \n",
    "    def output_error(self, l, output_error, z_list):\n",
    "        return np.dot(self.weights[-l+1].transpose(), output_error) * sigmoid_prime(z_list[-l])\n",
    "\n",
    "    def cost_weight(self, l, output_error, activation):\n",
    "        return np.dot(output_error, activation[-l-1].transpose())\n",
    "    \n",
    "    def update_network_weights(self, current_weight, learning_rate, mini_batch, mini_batch_weight):\n",
    "        return current_weight - (learning_rate/len(mini_batch)) * mini_batch_weight\n",
    "                                                         \n",
    "    def update_network_bias(self, current_bias, learning_rate, mini_batch, mini_batch_bias):\n",
    "        return current_bias - (learning_rate/len(mini_batch)) * mini_batch_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mnist_loader\n",
    "~~~~~~~~~~~~\n",
    "A library to load the MNIST image data.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Return the MNIST data as a tuple containing the training data,\n",
    "the validation data, and the test data.\n",
    "\"\"\"\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    The MNIST data set contains 50k entries, with each image being a\n",
    "    28*28 pixel array (total_pixels).\n",
    "    \"\"\"\n",
    "    total_pixels = 784\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    \n",
    "    X_train = [np.reshape(x, (total_pixels, 1)) for x in training_data[0]]\n",
    "    Y_train = [vectorized_result(y) for y in training_data[1]]\n",
    "    \n",
    "    X_validation = [np.reshape(x, (total_pixels, 1)) for x in validation_data[0]]\n",
    "    Y_validation = validation_data[1]\n",
    "    \n",
    "    X_test = [np.reshape(x, (total_pixels, 1)) for x in test_data[0]]\n",
    "    Y_test = test_data[1]\n",
    "    \n",
    "    return (X_train, Y_train, X_validation, Y_validation,  X_test, Y_test)\n",
    "\n",
    "\"\"\"\n",
    "Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "position and zeroes elsewhere.  This is used to convert a digit\n",
    "(0...9) into a corresponding desired output from the neural\n",
    "network.\n",
    "\"\"\"\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute and build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\tAccuracy : 81.48%\n",
      "Epoch #2\tAccuracy : 91.83%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8d3e293c2f58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-72f4f742f6e5>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, X_train, Y_train, X_validation, Y_validation, epochs, mini_batch_size, learning_rate, decay)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mmini_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmini_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch #\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-72f4f742f6e5>\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[0;34m(self, mini_batch, learning_rate)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mloss_func_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \"\"\"\n\u001b[1;32m     98\u001b[0m             \u001b[0mUpdates\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0mweight\u001b[0m \u001b[0mby\u001b[0m \u001b[0madding\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mrespective\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-72f4f742f6e5>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorized_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mz_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mactivation_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-72f4f742f6e5>\u001b[0m in \u001b[0;36mactivation\u001b[0;34m(self, w, a, b)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moutput_layer_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_list\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the training and validation data, create a Network object and run the SGD algorithm with it.\n",
    "Accuracy will be printed with the Epoch #.\n",
    "\"\"\"\n",
    "X_train, Y_train, X_validation, Y_validation,  X_test, Y_test= load_data()\n",
    "\n",
    "net = Network([784, 30, 10])\n",
    "net.SGD(X_train, Y_train, X_validation, Y_validation, 10, 10, 2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 92.09%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluate the network.\n",
    "\"\"\"\n",
    "net.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing (random grid search)\n",
    "For this script to run correctly, the following changes must be made to the Network class (replace current SGD and evaluate functions with below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(self, X_train, Y_train, X_validation, Y_validation, epochs, mini_batch_size, learning_rate, decay):\n",
    "    training_data = zip(X_train, Y_train)\n",
    "    validation_data = zip(X_validation, Y_validation)\n",
    "    training_data = list(training_data)\n",
    "    if validation_data:\n",
    "        validation_data = list(validation_data)\n",
    "        n_validation_data = len(validation_data)\n",
    "#       Updated for the testing\n",
    "#       ========================\n",
    "    mini_batches = []\n",
    "    high_score = [0,0]\n",
    "    for j in range(epochs):\n",
    "        random.shuffle(training_data)\n",
    "        for k in range(0, len(training_data), mini_batch_size):\n",
    "            mini_batches.append(training_data[k:k+mini_batch_size])\n",
    "        for mini_batch in mini_batches:\n",
    "            self.update_mini_batch(mini_batch, learning_rate)\n",
    "        if validation_data:\n",
    "            new_score = self.evaluate(X_validation, Y_validation)\n",
    "            if high_score[0] < new_score:\n",
    "                high_score[0] = new_score\n",
    "                high_score[1] = j + 1\n",
    "        learning_rate = learning_rate * (1-decay)\n",
    "    return high_score[0], high_score[1]\n",
    "#       ========================\n",
    "       \n",
    "def evaluate(self, X_test, Y_test):\n",
    "    test_data = zip(X_test, Y_test)\n",
    "    test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                    for (x, y) in test_data]\n",
    "#       Updated for the testing\n",
    "#       ========================\n",
    "    return (sum(int(x == y) for (x, y) in test_results) / 100)\n",
    "#       ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method tests all combinations inputted manually inside the script to try to replicate\n",
    "a random grid search method to find an optimum. The values inputted were backed up by \n",
    "literature and can be found in the document.\n",
    "This script writes to an external csv file which doesn't need to exist before calling, and if \n",
    "one does exist the values are appended, not overwritten. Stopping mid-script will lose any\n",
    "accumulated data as the table is only written to on the completion of the algorithm, this can\n",
    "be modified by putting the df.to_csv methods within the loop - after the df.append function.\n",
    "\"\"\"\n",
    "def random_grid_search():\n",
    "    # name the cols -- DON'T CHANGE\n",
    "    cols = [\"# of hidden layers\", \"# of total neurons\", \"list of layers\", \"epoch\", \"mini-batch size\", \\\n",
    "            \"learning rate\", \"learning rate decay\", \"Winning epoch\", \"Acc Valid\", \"Acc Test\"]\n",
    "    # create the data frame\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    ####### Change these values to test different combinations\n",
    "    epochs_test = [10]\n",
    "    mini_batch_size_test = np.arange(1, 100, 10).tolist()\n",
    "    learning_rate_test = np.arrange(0.25, 10, 0.25).tolist()\n",
    "    learning_rate_decay_test = [0.1, 0.01, 0.001]\n",
    "    network_shapes = [[784, 30, 5, 10],[784, 30, 10, 10], [784, 30, 30, 10]]\n",
    "    #######\n",
    "    \n",
    "    percentage = 0.0\n",
    "    X_train, Y_train, X_validation, Y_validation,  X_test, Y_test= load_data()\n",
    "\n",
    "    for e_idx, e in enumerate(epochs_test):\n",
    "        for mbs_idx, mbs in enumerate(mini_batch_size_test):\n",
    "            for lr_idx, lr in enumerate(learning_rate_test):\n",
    "                for lrd_idx, lrd in enumerate(learning_rate_decay_test):\n",
    "                    for network_shape in network_shapes:\n",
    "\n",
    "                        net = Network(network_shape)\n",
    "                        acc_valid, epoch_count = net.SGD(X_train, Y_train, X_validation, Y_validation, e, mbs, lr, lrd)\n",
    "                        acc_test = net.evaluate(X_test, Y_test)\n",
    "                        # append to the df \n",
    "\n",
    "                        n_neurons = sum(network_shape) - 794 # 798 is input + ouput layer which can't be changed\n",
    "                        n_hidden_layers = len(network_shape)-2\n",
    "\n",
    "                        df = df.append({\"# of hidden layers\": n_hidden_layers, \n",
    "                                        # Change this to lamda loop when we modify the actual shape etc\n",
    "                                        \"# of total neurons\": n_neurons,   \n",
    "                                        \"list of layers\": network_shape,        \n",
    "                                        \"epoch\" : e,                \n",
    "                                        \"mini-batch size\" : mbs,    \n",
    "                                        \"learning rate\" : lr,       \n",
    "                                        \"learning rate decay\" : lrd, \n",
    "                                        \"Winning epoch\" : epoch_count,\n",
    "                                        \"Acc Valid\": acc_valid,     \n",
    "                                        \"Acc Test\": acc_test},      \n",
    "                                       ignore_index=True)\n",
    "                         # sanity check\n",
    "                        percentage += (1.0 / len(epochs_test) / len(mini_batch_size_test) / len(learning_rate_test) \\\n",
    "                                       / len(learning_rate_decay_test) /len(network_shapes))\n",
    "                        print(\"Progress: \", '{0:.2f}'.format(percentage * 100), \"%\")\n",
    "\n",
    "    # name your file\n",
    "    csv_file = 'test_script_results.csv'\n",
    "\n",
    "    # write to file\n",
    "    '''If file exists, assume there is a header already'''\n",
    "    if os.path.isfile(csv_file):\n",
    "        df.to_csv(csv_file, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        df.to_csv(csv_file, mode='a', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
